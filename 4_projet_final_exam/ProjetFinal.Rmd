---
title: "Projet Final : Classification bayésienne et Analyse Factorielle Discriminante"
author: "Antoine LI, Lucas TONLOP"
date: "2025-2026"
output: 
  html_document: 
    toc: true
---



## Introduction
Ce projet vise à distinguer un texte écrit par un humain d’un texte généré par une IA, à partir d’un jeu de données issu du challenge Kaggle LLM-Detect. 
L’objectif est de construire un modèle de classification bayésienne, renforcé par une Analyse Factorielle Discriminante (AFD) afin de réduire la dimension et améliorer la séparation entre classes. 
Contrairement à nos premiers essais, nous utilisons ici le fichier **train_drcat_01.csv**, qui offre un volume de données plus important et un meilleur équilibre entre les classes.


```{r setup, include=FALSE}
# Ceci permet de ne pas afficher les messages d'avertissement et les messages d'information dans le rapport final
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  comment = NA
)
```



## 1. Chargement des données
Nous utilisons le fichier train_drcat_01.csv. L'utilisation de ce fichier est cruciale car elle permet au modèle d'apprendre sur un nombre d'exemples IA significatif.


```{r}
library(readr)  # lire des fichiers CSV
library(dplyr)  # manipulation de données

# Charger le fichier CSV
train <- read_csv("train_drcat_01.csv")

# Le fichier drcat contient généralement les colonnes 'text' et 'label'
# On renomme 'label' en 'target' pour plus de clarté
train <- train %>% rename(target = label)

# Premier aperçu
dim(train)
names(train)
head(train, 3)

# Vérifier la variable cible et son équilibre
# 0 = humain / 1 = IA
table(train$target)
prop.table(table(train$target))
```


Le fichier a été chargé avec succès. On constate que nous ne sommes plus face à un déséquilibre extrême : la classe IA représente désormais une part importante du dataset (environ 30 à 40% selon les versions de drcat), 
ce qui rend l'entraînement et l'évaluation statistique beaucoup plus robustes.


## 2. Données et prétraitement
Dans cette partie, Nous explorons la structure des textes et effectuons un nettoyage pour normaliser les données avant l'extraction stylométrique.


```{r}
library(ggplot2)  # visualisations
library(stringr)  # fonctions pratiques sur du texte

# Transformation de la cible en facteur et calcul des longueurs
train <- train %>%
  mutate(
    classe = factor(target, levels = c(0, 1), labels = c("Humain", "IA")),
    nb_caracteres = nchar(text),
    nb_mots = str_count(text, boundary("word"))
  )


# Résumé des longueurs moyennes par catégorie
train %>%
  group_by(classe) %>%
  summarise(
    n = n(),
    moy_mots = mean(nb_mots),
    moy_car = mean(nb_caracteres)
  )


# Visualisation de la répartition
ggplot(train, aes(x = classe, fill = classe)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Répartition des classes (Dataset DRCAT)", y = "Nombre de documents")

# Nettoyage rigoureux du texte
train <- train %>%
  mutate(
    text_clean = tolower(text),
    text_clean = str_replace_all(text_clean, "[0-9]+", " "),
    text_clean = str_replace_all(text_clean, "[[:punct:]]", " "),
    text_clean = str_squish(text_clean)
  )

```


L'analyse exploratoire montre que les longueurs de textes sont plus homogènes entre l'IA et l'Humain dans ce dataset. Le nettoyage permet d'éliminer le "bruit" (ponctuation, chiffres) pour se concentrer sur la structure pure du langage.



## 3. Extraction de caractéristiques
Dans cette partie, nous extrayons des variables de forme (stylométrie) et de complexité (lisibilité) pour nourrir nos modèles. 
L'extraction est réalisée sur l'intégralité du nouveau dataset (33 259 documents), ce qui assure une base statistique solide.


```{r}
# On installe/charge les packages nécessaires à la vectorisation et à la lisibilité
library(quanteda)
library(quanteda.textstats)
library(dplyr)
library(stringr)


# Calcul des statistiques de phrases (moyenne et écart-type de longueur)
sentence_stats <- function(txt) {
  s <- unlist(str_split(txt, "[.!?]+"))
  s <- str_trim(s[nchar(str_trim(s)) > 0])
  if (length(s) == 0) return(c(m_sent = 0, sd_sent = 0))
  sent_len <- str_count(s, boundary("word"))
  c(m_sent = mean(sent_len), sd_sent = sd(sent_len))
}


# Extraction stylométrique
sent_mat <- t(sapply(train$text, sentence_stats))
read_df <- textstat_readability(corpus(train, text_field = "text_clean"),
                                  measure = c("Flesch", "Flesch.Kincaid"))

# Compilation du tableau final des caractéristiques
stylo_df <- train %>%
  select(target, classe, nb_mots, nb_caracteres) %>%
  bind_cols(as.data.frame(sent_mat)) %>%
  bind_cols(read_df %>% select(Flesch, Flesch.Kincaid)) %>%
  na.omit()


# Aperçu
dim(stylo_df)
head(stylo_df, 3)
```
Les caractéristiques stylométriques ont bien été extraites. Nous disposons maintenant de `nrow(stylo_df)` lignes. 
Contrairement à notre première tentative, ce volume de données permet à l'AFD et au Naive Bayes de s'appuyer sur des distributions de probabilités réelles et non sur des cas isolés.

## 4. Analyse des facteurs discriminants
Ici, on va faire une AFD sur les variables stylométriques car elles sont peu nombreuses et interprétables.
On affichera l’axe discriminant (LD1) et les variables les plus discriminantes.


```{r}
library(MASS)
library(ggplot2)
library(dplyr)

# Entraînement de la LDA (AFD)
# Nous utilisons les variables stylométriques comme descripteurs
modele_afd <- lda(classe ~ nb_mots + nb_caracteres + m_sent + sd_sent + Flesch + Flesch.Kincaid, 
                  data = stylo_df)

# Projection des données sur le premier axe discriminant
pred_afd <- predict(modele_afd)
stylo_df$LD1 <- pred_afd$x[,1]

# Visualisation de la séparation
ggplot(stylo_df, aes(x = LD1, fill = classe)) +
  geom_density(alpha = 0.5) +
  theme_minimal() +
  labs(title = "Densité des scores sur l'axe discriminant LD1")

# Importance des variables
modele_afd$scaling
```
L'axe LD1 montre désormais une séparation visible entre les deux groupes (Humain vs IA). Contrairement à l'ancien dataset où les courbes de densité se chevauchaient presque totalement, on observe ici une distinction réelle.
L'examen des coefficients (scaling) montre que l'indice Flesch.Kincaid et le nombre de mots sont les variables les plus discriminantes. Cela suggère que l'IA et l'humain diffèrent significativement par la structure de leurs phrases et la complexité de leur vocabulaire sur ce dataset.


## 5. Classification bayésienne
Dans cette partie, nous entraînons un classifieur bayésien (Naive Bayes) en utilisant les axes discriminants
issus de l'AFD comme variables d'entrée. Puis nous réalisons une première évaluation sur un jeu de test.


```{r}
library(e1071)
library(dplyr)

set.seed(123)
# Découpage 80% train / 20% test
idx <- sample(1:nrow(stylo_df), 0.8 * nrow(stylo_df))
train_set <- stylo_df[idx, ]
test_set  <- stylo_df[-idx, ]

# Modèle Naive Bayes
modele_nb <- naiveBayes(classe ~ LD1, data = train_set)

# Prédiction sur le test
pred_nb <- predict(modele_nb, newdata = test_set)
table(Prediction = pred_nb, Realite = test_set$classe)
```
Le modèle parvient enfin à classer des documents dans les deux catégories. Les prédictions ne sont plus systématiquement "Humain", ce qui valide l'approche.



## 6. Évaluation et résultats
Dans cette partie, on évalue le modèle avec des métriques adaptées aux prédictions probabilistes et on met en place une validation croisée stratifiée.


```{r}
library(dplyr)
library(pROC)

# Matrice de confusion
conf_mat <- table(pred_nb, test_set$classe)
acc <- sum(diag(conf_mat)) / sum(conf_mat)
sensibilite <- conf_mat[2,2] / sum(conf_mat[,2]) # Recall IA

print(paste("Accuracy globale :", round(acc, 4)))
print(paste("Rappel (Recall) IA :", round(sensibilite, 4)))

# Courbe ROC
prob_nb <- predict(modele_nb, newdata = test_set, type = "raw")[,2]
roc_nb <- roc(test_set$classe, prob_nb)
plot(roc_nb, col="blue", main=paste("Courbe ROC - AUC =", round(auc(roc_nb), 3)))


```
Les résultats sont probants : l'accuracy est élevée et le rappel pour la classe IA est désormais significatif. 
Cela signifie que le modèle ne se contente pas de deviner la classe majoritaire, mais identifie réellement des patterns propres aux LLM.


## Conclusion
Ce projet démontre l'importance cruciale de la qualité et de l'équilibre des données en machine learning. En passant d'un dataset de 3 exemples IA à un dataset de plusieurs milliers (DRCAT), nous avons pu stabiliser nos modèles.

L'Analyse Factorielle Discriminante a permis de réduire la complexité stylométrique à un axe unique puissant, sur lequel le classifieur Naive Bayes a pu opérer avec succès. Le modèle est capable de distinguer les deux sources avec une fiabilité satisfaisante.

**Pistes d’amélioration :** <br>
Bien que performant, le modèle pourrait être encore amélioré en intégrant des n-grammes de mots (TF-IDF) pour capturer le vocabulaire spécifique, ou en utilisant des modèles plus complexes comme les forêts aléatoires. 
Toutefois, l'approche AFD + Bayes reste une méthode robuste et très interprétable pour la détection stylométrique.